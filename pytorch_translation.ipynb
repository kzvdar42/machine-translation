{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "torchtext_translation_tutorial (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vaMjWUZtg38R"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMu08_zMAY7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import tqdm\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "\n",
        "# Choose dataset\n",
        "# path = '/content/paracrawl-release1.en-ru.zipporah0-dedup-clean'\n",
        "path = 'corpus.en_ru.1m'\n",
        "\n",
        "def load_files(path):\n",
        "    res = ([], [])\n",
        "    for i, ext in enumerate(['.en', '.ru']):\n",
        "        with open(path + ext) as in_file:\n",
        "            res[i].extend(in_file.readlines())\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrtIPj4CAivz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57872c48-30c5-431f-ea5a-cd33aa091c2e"
      },
      "source": [
        "# Connect to drive for yandex dataset downloading.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_to_yandex_dataset = '/content/drive/My\\ Drive/To\\ delete/1mcorpus.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaMjWUZtg38R",
        "colab_type": "text"
      },
      "source": [
        "## Preparation\n",
        "Update/Install packages and download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YTpqDJioX9L",
        "colab_type": "code",
        "outputId": "b525d861-50a5-4820-b11f-3c371127e4fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Update torchtext\n",
        "!pip install torchtext -U\n",
        "# Install YouTokenToMe for tokenization\n",
        "!pip install youtokentome"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torchtext in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext) (0.1.85)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.0)\n",
            "Installing collected packages: youtokentome\n",
            "Successfully installed youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N9vzr78XYcE",
        "colab_type": "code",
        "outputId": "8bbfdee6-5503-4f11-c5ad-5fc6168d6c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Load Datasets\n",
        "\n",
        "# ParaCrawl dataset\n",
        "!curl https://s3.amazonaws.com/web-language-models/paracrawl/release1/paracrawl-release1.en-ru.zipporah0-dedup-clean.tgz --output paracrawl.tgz\n",
        "!tar -xvzf paracrawl.tgz\n",
        "# Yandex dataset\n",
        "os.system(f'cp {path_to_yandex_dataset} yandex.zip')\n",
        "!unzip yandex.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  637M  100  637M    0     0  33.9M      0  0:00:18  0:00:18 --:--:-- 36.5M\n",
            "paracrawl-release1.en-ru.zipporah0-dedup-clean.en\n",
            "paracrawl-release1.en-ru.zipporah0-dedup-clean.ru\n",
            "Archive:  yandex.zip\n",
            "  inflating: corpus.en_ru.1m.en      \n",
            "  inflating: corpus.en_ru.1m.ru      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmfrqBmzyNBY",
        "colab_type": "code",
        "outputId": "3383afa4-6d9f-4b21-aff4-2202f810845f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data_en, data_ru = load_files(path)\n",
        "\n",
        "raw_data = {'English' : [line for line in data_en], 'Russian': [line for line in data_ru]}\n",
        "\n",
        "df = pd.DataFrame(raw_data, columns=list(raw_data.keys()))\n",
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loAavSAPym44",
        "colab_type": "code",
        "outputId": "5bed518f-de0e-4ef1-b972-58d369a07648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df['en_len'] = df['English'].str.count(' ')\n",
        "df['ru_len'] = df['Russian'].str.count(' ')\n",
        "df = df.query('ru_len < 80 & en_len < 80')\n",
        "df = df.query('ru_len < en_len * 1.5 & ru_len * 1.5 > en_len')\n",
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(789101, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQEFSLt3zKx",
        "colab_type": "code",
        "outputId": "d880388c-9bae-48fb-a2b3-a596a0d50f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df = df[:50000]\n",
        "df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYYvRotrzRq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create train, test, val sets.\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "test, val = train_test_split(test, test_size=0.5)\n",
        "train.to_csv('yandex_train.csv', index=False)\n",
        "test.to_csv('yandex_test.csv', index=False)\n",
        "val.to_csv('yandex_val.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2CY1bO1hMVj",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZN-aua3JPTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import youtokentome as yttm\n",
        "vocab_size = 30000\n",
        "\n",
        "model_path = f'{path[:6]}_v{vocab_size}.tokenizer'\n",
        "if os.path.exists(model_path):\n",
        "    tokenizer = yttm.BPE(model=model_path)\n",
        "else:\n",
        "    data_en, data_ru = load_files(path)\n",
        "    temp_file_path = 'tokenizer_text.temp'\n",
        "    with open(temp_file_path, 'w') as out_file:\n",
        "        out_file.write('\\n'.join(map(str.lower, data_en)))\n",
        "        out_file.write('\\n'.join(map(str.lower, data_ru)))\n",
        "    tokenizer = yttm.BPE.train(data=temp_file_path, vocab_size=vocab_size, model=model_path)\n",
        "    # TODO: Delete temp file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UnqEDMXsd-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9509ebab-cdc3-48ca-da00-bff4f32b287e"
      },
      "source": [
        "def return_tokenizer_encoder(tokenizer, output_type='id'):\n",
        "    output_types = {'id':yttm.OutputType.ID, 'subword':yttm.OutputType.SUBWORD}\n",
        "    return lambda x: tokenizer.encode(x, output_type=output_types[output_type], bos=True, eos=True)\n",
        "\n",
        "SRC = Field(tokenize=return_tokenizer_encoder(tokenizer, 'subword'))\n",
        "TGT = Field(tokenize=return_tokenizer_encoder(tokenizer, 'subword'))\n",
        "\n",
        "data_fields = [('tgt', TGT), ('src', SRC)]\n",
        "(train_data,\n",
        " val_data,\n",
        " test_data) = TabularDataset.splits(path='./',\n",
        "                                    train='yandex_train.csv',\n",
        "                                    test='yandex_test.csv',\n",
        "                                    validation='yandex_val.csv',\n",
        "                                    format='csv', fields=data_fields,\n",
        "                                    skip_header = True)\n",
        "\n",
        "print(len(train_data), len(val_data), len(test_data))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000 5000 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07pvW18bpKIt",
        "colab_type": "code",
        "outputId": "3a6ebc7e-9b66-48ae-dd8d-c5c75fc18392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f\"src: {vars(train_data.examples[0])['src']}\")\n",
        "print(f\"tgt: {vars(train_data.examples[0])['tgt']}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: ['<BOS>', '▁-', '▁', 'Я', '▁си', 'роди', 'лец', '▁из', '?', '<EOS>']\n",
            "tgt: ['<BOS>', '▁\"', 'I', \"'m\", '▁a', '▁', 'C', 'y', 'ro', 'di', 'il', '▁from', '?', '<EOS>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGubBrNyn2cr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68127cc0-e1ba-4809-b57c-49e7fcb311b4"
      },
      "source": [
        "# TODO: Move to usage of `tokenizer` vocabulary\n",
        "SRC.build_vocab(train_data)\n",
        "TGT.build_vocab(train_data)\n",
        "print(len(SRC.vocab.stoi), len(TGT.vocab.stoi))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22770 15967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Ysl0QUn2c3",
        "colab_type": "text"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNztDWcjn2c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# FIX: iterator batches sentences of different lengths\n",
        "train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    sort_key=lambda x: (len(x.src), len(x.tgt)),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n",
        "\n",
        "data_iterators = {\n",
        "    'train': train_iterator,\n",
        "    'val': val_iterator,\n",
        "    'test': test_iterator,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQwdXgmPB36j",
        "colab_type": "code",
        "outputId": "bc5e2f1a-6753-43b3-8004-8ade3876c0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# Test iterator\n",
        "for _, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    tgt = batch.tgt\n",
        "    print('src shape:',src.shape)\n",
        "    print('tgt shape:', tgt.shape)\n",
        "    if (src > 4).any():\n",
        "        for s, t in zip(src.t()[:2], tgt.t()[:2]):\n",
        "            print([SRC.vocab.itos[token] for token in s])\n",
        "            print([TGT.vocab.itos[token] for token in t])\n",
        "    else:\n",
        "        print('Something is wrong')\n",
        "    break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src shape: torch.Size([78, 32])\n",
            "tgt shape: torch.Size([67, 32])\n",
            "['<BOS>', '▁', 'В', '▁следующем', '▁бло', 'ке', '▁мы', '▁указыва', 'ем', '▁как', '▁мы', '▁пере', 'писы', 'вались', '▁с', '▁бан', 'ком.', '<EOS>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<BOS>', '▁', 'I', 'n', '▁the', '▁next', '▁block', '▁we', '▁specify', '▁how', '▁we', '▁correspond', 'ed', '▁with', '▁the', '▁bank.', '<EOS>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<BOS>', '▁', 'В', 'ой', 'на', '▁-', '▁это', '▁мир,', '▁а', '▁свобода', '▁-', '▁это', '▁полицей', 'ское', '▁государство', '.', '<EOS>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<BOS>', '▁', 'W', 'ar', '▁is', '▁peace,', '▁and', '▁freedom', '▁is', '▁the', '▁police', '▁state', '<EOS>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYV4b6icn2dB",
        "colab_type": "text"
      },
      "source": [
        "Defining our ``nn.Module`` and ``Optimizer``\n",
        "----------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3y5Drcrn2dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntokens_src, ntokens_tgt, ninp, nhead, dim_feedforward, nlayers, pad_token, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import Transformer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.ninp = ninp\n",
        "        self.pad_token = pad_token\n",
        "        self.src_mask = None\n",
        "        # Token Encoders\n",
        "        self.src_encoder = nn.Embedding(ntokens_src, ninp)\n",
        "        self.tgt_encoder = nn.Embedding(ntokens_tgt, ninp)\n",
        "        # Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        # Transformer\n",
        "        self.transformer = Transformer(\n",
        "            d_model=ninp,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=nlayers,\n",
        "            num_decoder_layers=nlayers,\n",
        "            dropout=dropout,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "        )\n",
        "        self.out = nn.Linear(ninp, ntokens_tgt)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate matrix for seqential reveal of tokens.\"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.transformer._reset_parameters()\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "\n",
        "        src_key_padding_mask = (src == self.pad_token).bool().t()\n",
        "        tgt_key_padding_mask = (tgt == self.pad_token).bool().t()\n",
        "        memory_key_padding_mask = src_key_padding_mask.clone().detach()\n",
        "\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            self.src_mask = self._generate_square_subsequent_mask(len(src)).to(src.device)\n",
        "\n",
        "        src_enc = self.src_encoder(src) * math.sqrt(self.ninp)\n",
        "        src_enc = self.pos_encoder(src_enc)\n",
        "        tgt_enc = self.tgt_encoder(tgt) * math.sqrt(self.ninp)\n",
        "        tgt_enc = self.pos_encoder(tgt_enc)\n",
        "        output = self.transformer(src_enc, tgt_enc,\n",
        "                                  src_mask=self.src_mask,\n",
        "                                  src_key_padding_mask=src_key_padding_mask,\n",
        "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                                  memory_key_padding_mask=memory_key_padding_mask,\n",
        "                                  )\n",
        "        output = self.out(output)\n",
        "        del src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_enc, tgt_enc\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOgM82TGthyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens_src = len(SRC.vocab.stoi) # the size of vocabulary\n",
        "ntokens_tgt = len(TGT.vocab.stoi) # the size of vocabulary\n",
        "pad_token = SRC.vocab.stoi['<pad>']\n",
        "emsize = 256 # embedding dimension\n",
        "nhid = 128 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 6 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 8 # the number of heads in the multiheadattention models\n",
        "dropout = 0.5 # the dropout value\n",
        "model = TransformerModel(ntokens_src, ntokens_tgt, emsize, nhead, nhid, nlayers, pad_token, dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNl9Nrjon2dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore padding index during the loss computation.\n",
        "PAD_IDX = TGT.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "lr = 1.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5.0, gamma=0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9apm7uen2dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, criterion, optimizer, data_iterator, is_train_phase, desc):\n",
        "    if is_train_phase:\n",
        "        model.train() # Turn on the train mode\n",
        "    else:\n",
        "        model.eval()\n",
        "    total_loss = 0.0\n",
        "    pbar = tqdm.tqdm(total=len(data_iterator), desc=desc, position=0, leave=True)\n",
        "    for i, batch in enumerate(data_iterator):\n",
        "        src, tgt = batch.src, batch.tgt\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(is_train_phase):\n",
        "            output = model(src, tgt).transpose(1, 2)\n",
        "            loss = criterion(output, tgt)\n",
        "\n",
        "            if is_train_phase:\n",
        "                loss.backward()\n",
        "                # Clip gradient to deal with gradient explosion\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(desc + f'- loss: {total_loss / (i+1):7.4}')\n",
        "        # Delete all new vars due to memory leak\n",
        "        del batch, src, tgt, output, loss\n",
        "    return model\n",
        "\n",
        "def train_model(model, n_epochs, data_iterators, criterion, optimizer, scheduler):\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f'------------ Epoch {epoch} ------------')\n",
        "        for phase in ['train', 'val']:\n",
        "            desc = f'{phase.title()} Epoch #{epoch} '\n",
        "            run_model(model, criterion, optimizer,\n",
        "                      data_iterators[phase], phase == 'train',\n",
        "                      desc)\n",
        "        scheduler.step()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXr1J0fQ_TP9",
        "colab_type": "code",
        "outputId": "190a0db2-bc96-4485-e9d4-5030fd73f78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_epochs = 3\n",
        "model = train_model(model, n_epochs, data_iterators,\n",
        "                    criterion, optimizer, scheduler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rTrain Epoch #0 :   0%|          | 0/1250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------ Epoch 0 ------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch #0 - loss: 0.007051:   2%|▏         | 27/1250 [00:10<07:33,  2.69it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtY1RCJnLQA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(model, text, max_len=80, custom_string=False):\n",
        "    model.eval()\n",
        "    # Prepare text\n",
        "    src = SRC.preprocess(text)\n",
        "    print(len(src))\n",
        "    print([token for token in src])\n",
        "    src = SRC.process([src], device=device)\n",
        "    input_pad = SRC.vocab.stoi['<pad>']\n",
        "    src_mask = (src != input_pad)\n",
        "    # Run encoder\n",
        "    src_enc = model.src_encoder(src) * math.sqrt(model.ninp)\n",
        "    src_enc = model.pos_encoder(src_enc)\n",
        "    e_outputs = model.transformer.encoder(src_enc, \n",
        "                                          src_mask\n",
        "                                          )\n",
        "    \n",
        "    # Prepare tensor for answers\n",
        "    outputs = torch.zeros(max_len).type_as(src.data)\n",
        "    # Set the first token as '<sos>'\n",
        "    outputs[0] = torch.LongTensor([TGT.vocab.stoi['<BOS>']])\n",
        "    for i in range(1, max_len):\n",
        "        tgt_mask = model._generate_square_subsequent_mask(i).to(device)\n",
        "        outputs_enc = outputs[:i].unsqueeze(1)\n",
        "        outputs_enc = model.tgt_encoder(outputs_enc)\n",
        "        outputs_enc = model.pos_encoder(outputs_enc)\n",
        "\n",
        "        d_out = model.transformer.decoder(outputs_enc, e_outputs,\n",
        "                                          tgt_mask=tgt_mask,\n",
        "                                          )\n",
        "        out = model.out(d_out)\n",
        "        out = F.softmax(out, dim=-1)\n",
        "        _, ix = out[:, -1].data.topk(1)\n",
        "        \n",
        "        outputs[i] = ix[0][0]\n",
        "        # if ix[0][0] == TGT.vocab.stoi['<EOS>']:\n",
        "        #     break\n",
        "    return ' '.join([TGT.vocab.itos[ix] for ix in outputs[1:i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGPIqW8GNLrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f3a64c6b-71ce-4fe7-81c4-dc9e83e7c60c"
      },
      "source": [
        "translate(model, train_data[1].__dict__['src'])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "['<BOS>', '▁', 'С', 'мер', 'тная', '▁каз', 'нь', '▁за', '▁измен', 'у.', '<EOS>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enitOd_CNx8k",
        "colab_type": "code",
        "outputId": "8b9a70e9-315e-467f-f586-142355ce5446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "translate(model, 'Машинное обучение это здорово!')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "['<BOS>', '▁', 'М', 'а', 'ши', 'нное', '▁обучение', '▁это', '▁здоро', 'во', '!', '<EOS>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS> <BOS>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}